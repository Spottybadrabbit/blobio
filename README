Synopsis:
	A trivial protocol to manage immutable blobs over a network.
Usage:
	#  get a blob
	$ nc blob.setspace.com 1797
	> get sha:a0bc76c479b55b5af2e3c7c4a4d2ccf44e6c4e71
	< To be, or not to be : that is the question:
	< Whether 'tis nobler in the mind to suffer
	< The slings and arrows of outrageous fortune,
	< ...
	<EOT>

	#  put a blob to network
	$ echo 'hello, world' | shasum
	cd50d19784897085a8d0e3e413f8612b097c03f1  -
	$ nc blob.setspace.com 1797
	> put sha:cd50d19784897085a8d0e3e413f8612b097c03f1
	> hello, world
	< ok
	<EOT>

	# take a blob
	$ nc blob.setspace.com 1797
	> take sha:cd50d19784897085a8d0e3e413f8612b097c03f1
	< ok
	< hello, world
	> ok
	< ok
	<EOT>

	# does the blob still exist?
	$ nc blob.setspace.com 1797
	> get sha:cd50d19784897085a8d0e3e413f8612b097c03f1
	< no
	<EOT>

Description:
	The blobio environment implements a simple client/server protocol for 
	associating crypto hash digests as keys for immutable, binary large
	objects stored on a network.  The blobs are referenced with a uri
	(uniform resource identifier) called a uniform digest, shortened to
	'udig', with a syntax like

		algorithm:digest

	where 'algorithm' is the hash algorithm, like 'sha' for example, and
	'digest' is the ascii hash value of the blob.  The colon character
	separates the algorithm and the digest.  The algorithm matches the
	perl5 regex
	
		^[a-z][a-z0-9]{0,7}$

	and the digest matches

		^[[:isgraph:]]{32,128}$
		^[[:isascii:]{32,128}$

	An example udig is sha:cd50d19784897085a8d0e3e413f8612b097c03f1
	which describes the string "hello, world\n".

		echo 'hello, world' | shasum
		cd50d19784897085a8d0e3e413f8612b097c03f1  -

	The network protocol implements 7 verbs
	
		get <udig>		get a blob with a particular digest
		put <udig>		write a blob with a digest
		take <udig>		get a blob that the server may forget
		give <udig>		put a blob that the client may forget
		eat <udig>		verify a blob exists on server

		wrap			bundle current unwrapped brr traffic
					logs into a single blob and return
					udig of the set of all blobs wrapped
					after the previous roll. the wrapped
					<udig>, returned to client, will
					be in the next wrap, which allows
					perpetual chaining.

		roll <udig>		forget all wrapped BRR logs in the
					udig set described by <udig>.
					subsequent wrap/rolls will probably
					never see these BRR sets again.
					however, the blobs themselves will exist
					and be gettable.  the <udig> of the
					rolled udig will be in the next wrap set
					as a "roll" blob request record.

	and replys comprised of sequences of 'ok' and 'no'.

	On the blob server, named "biod", for each correct client request a
	single record describing the request is written to the file

		server:$BLOBIO_ROOT/spool/biod.brr

	Those records are named "Blob Request Records" and abbreiviated as BRR.

	The format of a single request record is a new-line terminated list of
	tab separated fields, i.e, the typical unixy ascii record:

		start_time		#  YYYY-MM-DDThh:mm:ss.ns[+-]HH:MM
		netflow			#  [a-z][a-z0-9]{0,7}~[[:graph:]]{1,128}
		verb			#  get/put/take/give/eat/wrap/roll
		algorithm:digest	#  udig of the blob in request
		chat_history		#  ok/no handshake between server&client
		blob_size		#  unsigned 64 bit long < 2^63
		wall_duration		#  request wall duration in sec.ns>=0

	Most important is that each client request is associated with a single
	blob described by a udig. Also, BRR records are always syntactally
	correct as described above.  For example, a request for a syntactically
	incorrect udig or an unknown verb will not generate a BRR record.

	The maximum size of a blob request record is 371 bytes.
	
		(10+1+8+1+9+1+5) + 1 +	=   start request time
		(8+1+128) + 1 +		>=  netflow
		(8) + 1 +		>=  verb
		(8+1+128) + 1 +		>=  algorithm:digest
		(8) + 1 + 		>=  chat history: max=ok,ok,ok
		(20) + 1 +		>=  blob size
		(10+1+9)		>=  wall duration

		<= 371 bytes

	not including a trailing new-line spool/biod.brr or null character.

Protocol Flow:
 	>get udig\n			# request for blob by udig
 	    <ok\n[bytes][close]		#   server sends bytes of blob
            <no\n[close]		#   server rejects request

  	>put udig\n[bytes]		# put udig and blob bytes
  	    <ok\n[close]		#   server accepts blob bytes
            <no\n[close]		#   server rejects request

  	>take udig\n			# request for blob by udig
  	    <ok\n[bytes]		#   server sends blob bytes
  		>ok\n			#     client has the blob
  		    <ok[close]		#       server forgets the blob
  		    <no[close]		#       server may not forget the blob
  		>no[close]		#     client rejects blob
  	    <no\n[close]		#   server rejects request

  	>give udig\n[bytes]		# client sends the udig and blob bytes
  	    <ok\n			#   server accepts the blob
  	        >ok[close]		#     client forgets blob
  		>no[close]		#     client might remember the blob
  	    <no\n[close]		#   server rejects blob request

	>eat udig\n			# client requests to verify blob
	    <ok\n			#   blob exists and has been verified
	    <no\n			#   server was unable to digest the blob

	>wrap				# request udig set of traffic logs
	    <ok\nudig\n[close]		#   udig of set of traffic logs
	    <no\n[close]		#   no logs available

	>roll udig			# udig set of traffic logs to forget
	    <ok\n[close]		# logs forgotten by server.  typically
	    				# a wrap set.
	    <no\n[close]		# not all logs in set forgotten

	Warning: the protocol is ambiguous about how the end of the blob stream
	is physically interpreted.  This may turn out to be a serious design
	flaw, particulary with regard to denial-of-service attacks.  Essentially
	the only indication that the end of the blob has been reached is that 
	the socket has closed;  nothing in the protocol explicity indicates
	"END of BLOB", other than the sequence of bytes matching the signature.
	In other words, only the digest presented by the request determines
	the correctness of the blob.

	As blobio evolves, I (jmscott) expect a client friendly, RESTful and
	inherently more complex caching protocol to be built in front of the
	blobio protocol described above.  In other words, a biod server never
	talks to untrusted code, due to the denial of service susceptibility
	described above.

Protocol Questions:
	- what about forbidding ':' as a digest character?

	- should 'roll' with no udig return the last rolled udig?

        - think about a "prove" command that allows the server to formally prove
	  the existence of a set of blobs, thus verifying the server is not
	  lying about storage.  the obvious technique would be for the client
	  to send the udig of a randomly selected set of blobs for which the
	  server is expected to digest the cancatenation of the blobs.

	- reimplement wrap as

		wrap <udig>

	  the idea is that the final record of the wrap set will always contain
	  <udig>, creating a kind of audit trail.  also insures no wrap set is
	  empty and sequential wraps could always return different udigs if the
	  given udig was unique. in shell parlance

	  	test $(blobio wrap <udigA>) = $(blobio wrap <udigB>)

	  is always false, even when udigA = udigB.

Billing Model Questions:
	- what to measure per billing cycle:
		total count of blobs stored
		total bytes stored
		total bytes transfered on public interface
		avg transfer bytes/duration > X
			for blob size > Y
		total number of eat requests/proof of retrievability

		should a small numbers of plans exist or should a formula
		determine cycle charges?

		what about overages? a mulligan is nice for customer
		satisfaction but rolling to next higher plan is draconian.

	- duration of storage: per blob or per plan?

	- should the blob request record include a "duration to first byte",
	  which measures the difference between start_time and time after the
	  first read or write to the client.  the time to first byte can be
	  observed on the network, sort of.  the sender will see a slightly
	  different time to first byte than the receiver, as with the
	  start_time.

Memes:
	- name the get/put/give/take/wrap/roll/eat protocol 'blob.io'.

	- de-emphasize key/value, emphasize historical flow of events.
	   never rediscover of facts is silly

	- blob.io gives perpetual accuracy, rigorous participants in protocol,
	   proof of retrievability
To Do:
	- Should the variable BLOBIO_ROOT be renamed BLOBIO_INSTALL or
	  BLOBIO_DIST?  Currently BLOBIO_ROOT means the actual install 
	  directory and not the parent, which is the correct root.

	- in biod is a three second socket accept timeout to quick?

	- shasum on mac os is about 3 times quicker than 'blobio eat'. why?

	- add option to biod to limit size of accepted blob

	- what about adding system/user duration to the brr record?
	  need to do now.

	- investigate linux splice() system call.

	- create postgres types for at least brr timestamp and netflow.

	- eliminate the sk: algorithm in a udig.  screws up case dependent
	  mac os file system

	- think about separate biod processes sharing the same data/ file system
	  but otherwise separate.  or, explore union file systems.

	- biod does not always remove run/biod.pid

	- make the postgresql udig data type be an extension

	- prove that every brr log contains a successful "wrap" other than the
	  the first brr log.  critical to prove this.

	- two quick wraps in a row could generate the same wrap set?

	- should BRR log files always have unique digests, modulo other servers.

	- contemplate running the server such that the empty blob always exists

	- in biod server, investigate closing down all unused file descriptiors
	  upon forking a blob request

	- what about a stats process to handle rolling, distinct udig count
	  and blob byte count?

	- think about adding fadvise to file system blob reads.
          seems natural since every open of blob file will read the entire blob.

	- rename biod-logger/brr-logger/arborist processes to shorter
		biod-log, biod-brr-log, biod-arbor

	- rethink how stats are delivered to snmp or rrd graphs - udp service?

	- cleanup the code in server/sha_fs.c

	- create an is_empty() function in postgres/udig and in blobio client.

	- add support for unix domain socket, std{in,out}, inetd, and libevent

	- upon server start up on mac os x test for case sensitive file system

	     see http://www.mail-archive.com/wine-devel@winehq.org/msg66830.html

	  another idea under macosx would be to convert blob file path to lower
	  case.

	- rewrite makefile for client only install, without golang requirement

	- should the postgres datatype include a function is_zero_size(udig)?

	- the postgres udig data type ought to be in the blobio schema.

	- merge fdr2sql & cron-fdr2pg into flowd

	- enable/disable specific commands get/put/give/take per server/per
	  subnet

	- trivial bandwidth throttling

	- move log/*.[fx]dr files to spool

	- named pipes to flowd as another tail source

	- signal handling needs to be pushed to main listen loop or cleaned
 	  up with sigaction().

Blame:
 	jmscott@setspace.com
 	setspace@gmail.com
